{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from [DeepLizard](https://deeplizard.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move backwards starting from the output layer and the loss function(SGD) cacluates the loss between actual values and preidcted and updates the weight accordingly at the output later    \n",
    "ie if it is classification problem , then one of the node's value will increase which ever is the actual label and rest all values will be decreased.   \n",
    "This process continues until we reach the input layer where we do not modify anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps are :\n",
    "- Pass data to model via forward propogation (forward pass)\n",
    "- Calculate loss on output\n",
    "- SGD minimizes the loss\n",
    "    - By calculating the gradient of the loss function and updating the weights\n",
    "    - Gradient is calcualted via backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/notations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/notation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstable Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Gradient we mean the gradient of loss with respect to weights.   \n",
    "This is calculated using backpropogation   \n",
    "After that we (SGD or any optimizer) update the weights using gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes in the early layers of network the weights become very small (less than 1) , even if we update the weights  that does not have much effect and the network stops to learn , basically yhe weight vanishes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier a weight resides in the network the more dependency it has in the network , becasue of chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploding is exactly opposite (greater than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the are problems in traning nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions - possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While randomly initializing the weight we try to keep the distribution normal ie mean = 0 and sd =1   \n",
    "To solve this problem while initializing we can force the variance to be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ var(weights) = 1/n $ , where n = no of connected nodes from the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initialization is called xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16 , input_shape = (1,5) , activation='relu'),\n",
    "    Dense(32 , activation='relu', kernel_initializer='glorot_uniform'), # xavier inititalizer , by default same is used\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is bias in NN ?\n",
    " - Each neuron has a bias ( so in an network there are many baises )\n",
    " - Each is learnable just like weights\n",
    " - Optmizer updates the bias as well while updating the weights (for example SGD is optimiizer)\n",
    " - Bias can be thought as threshold \n",
    " - Bias determines if a neuron is activated and by how much\n",
    " - Introducing biases increases the flexibility of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/bias1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here without bias the activation function will not fire because it become zeros, but if we wanted the threshold to increase  and not be 0 , then we can introduce the bias here.  \n",
    "After introducing bias the activation fires as output os 0.65 (-0.35 + 1) and relu(0.65) = 0.65 , hence this gets activated after introducing  bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done in opposite direction as well,  when we do not want to fire a neuron , eg is we want to activate the neuron only when the output is moree than 5 , then the bias will be -5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NN these biases are updated automatically during traning as the model learns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable parameter in Fully Connected  NN - Eg Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a learnable parameter ?\n",
    "- A parameter that is learned during training (trainable paramters)\n",
    "- weights and biases \n",
    "\n",
    "How is the number of learnable parameters calcualted ? \n",
    "- We calculate for each layer and then sum up for all the layers\n",
    "  - input , output , biases - for dense layers for cnn there is diff type\n",
    "  - formula ->  $inputs * outputs + biases$. The same need to be done for all the layers in network and then sum up to get total learnable parameters in network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/learnable_dense_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer = 0 input parameters as these are labels /values   \n",
    "       \n",
    "Hidden Layer - input = 2 parameters     \n",
    "                output = 3 parameters     \n",
    "                bias = 3     \n",
    "              formula = $(2*3) + 3)$ = 9\n",
    "              \n",
    "Output Layer input = 3 parameters\n",
    "                output = 2 parameters    \n",
    "                bias = 2 parameters\n",
    "               formula = $(3*2) + 2$ = 8\n",
    "               \n",
    "Total parameters in network =  $ 9+8 = 17 $ learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable parameter in CNN Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers have additional **filter** which dense layers do not have ,also the size of filter matters.   \n",
    "The input to the layer is dependent on the previous layer and its type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/learnable_param_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/cnn_learn_param2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation\n",
    "\n",
    "Input layer\n",
    "- 0 parameters\n",
    "\n",
    "1st conv layer\n",
    "- input param = 3\n",
    "- filters = 2 \n",
    "- size of filter = $3*3$\n",
    "- bias = no of filter = 2\n",
    "- total params = 3 * (3*3 * 2) + 2  = 56 (input * filter_size * no_of_filter + bias )\n",
    "\n",
    "2nd conv layer\n",
    "- input param = 2 (no of filter from previous layer)\n",
    "- filters = 3\n",
    "- size of filter = $3*3$\n",
    "- bias = no of filter = \n",
    "- total params = 2 * (3*3 * 3) + 3  = 57 (input * filter_size * no_of_filter + bias )\n",
    "\n",
    "Note : **Before passing o/p from conv layer to dense layer, we have to flatten the o/p**\n",
    "\n",
    "Here it is image data (20*20*3) , 3 is filter , and the network uses zero padding \n",
    "Output layer\n",
    "- input param = 20 *20 *3 = 1200\n",
    "- output param = 2 , only 2 nodes are present\n",
    "- size of filter = $3*3$\n",
    "- bias = no of filter = \n",
    "- total params = 1200 * 2 + 2 = 2402 (input * nodes +  bias )\n",
    "\n",
    "\n",
    "total leranable params = 45 + 57 + 2402 = 2515 params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is technique that helps reduce overfitting\n",
    "- It penalizes for complexity\n",
    "- the most common way to use regularization is to add it to the loss for larger weights\n",
    "- We generally use regularization to reduce the weights to optimize the objective of minimising the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 , L2 Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Norm - The length of the vector is reffered as the vector norm or the vector's magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16 , input_shape = (1,) , activation='relu'),\n",
    "    Dense(32 , activation='relu', kernel_regularizer = regularizers.l2(0.01)), #  regularizer  set per layer basis\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Size is the number of samples that will be passed through network at one time.\n",
    "- A batch is also called as **mini-batch** \n",
    "- Larger batches = Faster Training but the quality of model may degrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying batch size in model\n",
    "\n",
    "model.fit(...... , batch_size =10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "- Transfer Learning - gain knowledge while solving one problem ans applying to solve other\n",
    "- Fine tuning is utlilizing transfer learning - using existing model without building it from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the existing model and remove the last layer , which was classifying whether an image is of car or not and add the ouput layer to classify for truck instead of cars.  \n",
    "Sometimes we may need to remove more than 1 layers. and add more than 1 layers.\n",
    "- this depends on how similar the task is for each of the models\n",
    "- generally layers at the beginning learn more generic features like edges and lines, layers at the end are more specific\n",
    "\n",
    "We need to freeze the older layer weights for initial layers (ie we don't want to update the weights for initial layers only on the new or modified layers it should update)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization (Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize = Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize \n",
    "- for numerical data points getting it into lower scale , like 10-1000 to 0-1\n",
    "\n",
    "Standardize\n",
    "- $z = (x-m)s$ , forces  the sd to be 1 and mean  = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boils down to putting the data to known or standard scale , trying to get all features on scale will reduce the chance of model being unstable (exploding gradient problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Norm is applied to a layer to make sure one of the node's output does not become very large and make the network unstable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    " - Normalize the o/p from activation function $z = (x-m)/s$ ,  s = SD , m = mean , x= actual value\n",
    " - Multiply normalized output by arbitrary parameter g  i.e. $(z*g)$\n",
    " - Add arbitrary parameter b to resulting product  i.e. $(z*g)+b$\n",
    "   - all these parameters are traninable  (m, s, g, b) ie they will also get optimized during training\n",
    "   - this is done so that the weights do not become very large and imbalance the detwork\n",
    "   - this increases the speed of the network\n",
    "   - Batch normalizes the output from activation function inside the layers comapred to regular normalization which occurs before the input to input layer\n",
    "   - Also this occurs per batch basis , hence the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16 , input_shape = (1,5) , activation='relu'),\n",
    "    Dense(32 , activation='relu'),\n",
    "    BatchNormalization(axis=1), # BatchNormalized  following the layer for which we want the o/p  to be normalized\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37764bitmlcondab0e91abca1e444e6ba4cae782abe292d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
