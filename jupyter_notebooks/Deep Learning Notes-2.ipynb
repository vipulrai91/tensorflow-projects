{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from [DeepLizard](https://deeplizard.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move backwards starting from the output layer and the loss function(SGD) cacluates the loss between actual values and preidcted and updates the weight accordingly at the output later    \n",
    "ie if it is classification problem , then one of the node's value will increase which ever is the actual label and rest all values will be decreased.   \n",
    "This process continues until we reach the input layer where we do not modify anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps are :\n",
    "- Pass data to model via forward propogation (forward pass)\n",
    "- Calculate loss on output\n",
    "- SGD minimizes the loss\n",
    "    - By calculating the gradient of the loss function and updating the weights\n",
    "    - Gradient is calcualted via backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/notations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/notation_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstable Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Gradient we mean the gradient of loss with respect to weights.   \n",
    "This is calculated using backpropogation   \n",
    "After that we (SGD or any optimizer) update the weights using gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes in the early layers of network the weights become very small (less than 1) , even if we update the weights  that does not have much effect and the network stops to learn , basically yhe weight vanishes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier a weight resides in the network the more dependency it has in the network , becasue of chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploding is exactly opposite (greater than 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the are problems in traning nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions - possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While randomly initializing the weight we try to keep the distribution normal ie mean = 0 and sd =1   \n",
    "To solve this problem while initializing we can force the variance to be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ var(weights) = 1/n $ , where n = no of connected nodes from the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initialization is called xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16 , input_shape = (1,5) , activation='relu'),\n",
    "    Dense(32 , activation='relu', kernel_initializer='glorot_uniform'), # xavier inititalizer , by default same is used\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is bias in NN ?\n",
    " - Each neuron has a bias ( so in an network there are many baises )\n",
    " - Each is learnable just like weights\n",
    " - Optmizer updates the bias as well while updating the weights (for example SGD is optimiizer)\n",
    " - Bias can be thought as threshold \n",
    " - Bias determines if a neuron is activated and by how much\n",
    " - Introducing biases increases the flexibility of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/bias1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here without bias the activation function will not fire because it become zeros, but if we wanted the threshold to increase  and not be 0 , then we can introduce the bias here.  \n",
    "After introducing bias the activation fires as output os 0.65 (-0.35 + 1) and relu(0.65) = 0.65 , hence this gets activated after introducing  bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done in opposite direction as well,  when we do not want to fire a neuron , eg is we want to activate the neuron only when the output is moree than 5 , then the bias will be -5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NN these biases are updated automatically during traning as the model learns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable parameter in NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a learnable parameter ?\n",
    "- A parameter that is learned during training (trainable paramters)\n",
    "- weights and biases \n",
    "\n",
    "How is the number of learnable parameters calcualted ? \n",
    "- We calculate for each layer and then sum up for all the layers\n",
    "  - input , output , biases - for dense layers for cnn there is diff type\n",
    "  - formula ->  $inputs * outputs + biases$. The same need to be done for all the layers in network and then sum up to get total learnable parameters in network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/learnable_dense_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer = 0 input parameters as these are labels /values   \n",
    "       \n",
    "Hidden Layer - input = 2 parameters     \n",
    "                output = 3 parameters     \n",
    "                bias = 3     \n",
    "              formula = $(2*3) + 3)$ = 9\n",
    "              \n",
    "Output Layer input = 3 parameters\n",
    "                output = 2 parameters    \n",
    "                bias = 2 parameters\n",
    "               formula = $(3*2) + 2$ = 8\n",
    "               \n",
    "Total parameters in network =  $ 9+8 = 17 $ learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37764bitmlcondab0e91abca1e444e6ba4cae782abe292d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
